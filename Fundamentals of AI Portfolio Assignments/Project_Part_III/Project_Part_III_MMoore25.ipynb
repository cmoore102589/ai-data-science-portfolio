{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmoore102589/ai-data-science-portfolio/blob/main/Fundamentals%20of%20AI%20Portfolio%20Assignments/Project_Part_III/Project_Part_III_MMoore25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COSC-640: Project (Part III)\n",
        "\n",
        "**Matthew Corley Moore**\n",
        "\n",
        "[PLACEHOLDER_FOR_NOTEBOOK_LINK]\n"
      ],
      "metadata": {
        "id": "V1mEMoEjs7-k"
      },
      "id": "V1mEMoEjs7-k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "Follow the instructions below to copy this notebook and to perform some initial setup.\n",
        "\n",
        "1. Copy this notebook by selecting `File > Save a copy in Drive`.\n",
        "2. A new window should open for the copied notebook. Move the new notebook to your course folder in Google Drive by selecting `File > Move` and then selecting the desired folder.\n",
        "3. Update the name of the notebook by removing \"Copy of\" and replacing \"Username\" with your actual username.\n",
        "4. Update the first cell in the notebook by replacing \"**Student Name**\" with your actual name.\n",
        "5. Do not edit the line that says `PLACEHOLDER_FOR_NOTEBOOK_LINK`. This will be used by the [Notebook Renderer](https://colab.research.google.com/drive/1CJTipys46ldZxJFwnt7XbdjQUfkmoXeU?usp=sharing) tool to insert a link to your Colab notebook.\n",
        "6. Enable link sharing for your notebook."
      ],
      "metadata": {
        "id": "yqW7TPzztA7-"
      },
      "id": "yqW7TPzztA7-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Colab Environment\n",
        "\n",
        "Run the cell below to download the `aitools` course package."
      ],
      "metadata": {
        "id": "9zVmA1ybtDK7"
      },
      "id": "9zVmA1ybtDK7"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm aitools -r\n",
        "!git clone https://github.com/drbeane/aitools.git"
      ],
      "metadata": {
        "id": "Yl3P8M3ItHtc"
      },
      "id": "Yl3P8M3ItHtc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "844aa959-1da9-4f19-8411-84df07000dc5",
      "metadata": {
        "id": "844aa959-1da9-4f19-8411-84df07000dc5"
      },
      "source": [
        "Run the cell below to import the necessary tools for this assignment. No other import statements are required for this lab, and no other import statements should be included in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "550c7469-eb57-4dca-8e1a-fc0d260e510e",
      "metadata": {
        "id": "550c7469-eb57-4dca-8e1a-fc0d260e510e"
      },
      "outputs": [],
      "source": [
        "from aitools.envs import BotPlayerEnv\n",
        "from aitools.algs import PolicyPlayer, RandomPlayer, MinimaxPlayer\n",
        "from aitools.algs import TDAgent\n",
        "from aitools.algs import play_game, tournament"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description of Part III of Project\n",
        "\n",
        "In Part III of the project, you will train Q-learning agent to play Nim. The agent will be trained by playing thousands of games against a `RandomPlayer` agent, but will eventually be able to consistently defeat the better playing `MininmaxPlayer` agents."
      ],
      "metadata": {
        "id": "8rka4DqBvCOF"
      },
      "id": "8rka4DqBvCOF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define `Nim` Class\n",
        "\n",
        "Copy the definition for the `Nim` class from Part I  of the project into the cell below."
      ],
      "metadata": {
        "id": "4eqL4ltIhIRd"
      },
      "id": "4eqL4ltIhIRd"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Nim:\n",
        "\n",
        "    def __init__(self, piles, stones, limit):\n",
        "        self.piles = piles\n",
        "        self.stones = stones\n",
        "        self.limit = limit\n",
        "        self.board = [stones] * piles\n",
        "        self.turns = 0\n",
        "        self.cur_player = 1\n",
        "        self.winner = None\n",
        "\n",
        "    def display(self):\n",
        "      print(f\"Current Turn: {self.turns}, Current Player: {self.cur_player}, Piles: {self.board}\")\n",
        "\n",
        "    def copy(self):\n",
        "        new_instance = Nim(self.piles, self.stones, self.limit)\n",
        "        new_instance.board = self.board.copy()\n",
        "        new_instance.turns = self.turns\n",
        "        new_instance.cur_player = self.cur_player\n",
        "        new_instance.winner = self.winner\n",
        "        return new_instance\n",
        "\n",
        "    def check_for_win(self):\n",
        "        if all(pile == 0 for pile in self.board):\n",
        "          self.winner = self.cur_player\n",
        "\n",
        "    def get_actions(self):\n",
        "        actions = []\n",
        "        for pile_index, stones in enumerate(self.board):\n",
        "            max_stones = min(self.limit, stones)\n",
        "            for stones_to_take in range(1, max_stones + 1):\n",
        "                actions.append((pile_index, stones_to_take))\n",
        "        return actions\n",
        "\n",
        "    def take_action(self, a):\n",
        "        new_node = self.copy()\n",
        "        pile_index, stones_removed = a\n",
        "        new_node.board[pile_index] -= stones_removed\n",
        "        new_node.turns += 1\n",
        "        new_node.cur_player = 2 if new_node.cur_player == 1 else 1\n",
        "        new_node.check_for_win()\n",
        "        return new_node\n",
        "\n",
        "    def get_state(self):\n",
        "        s = 0\n",
        "        for i in range(self.piles):\n",
        "            s += self.board[i] * (self.stones + 1) ** i\n",
        "        return s\n",
        "\n",
        "    def heuristic(self, agent, mode=None):\n",
        "        return 0"
      ],
      "metadata": {
        "id": "geVSCByLhSR4"
      },
      "execution_count": null,
      "outputs": [],
      "id": "geVSCByLhSR4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Classes\n",
        "\n",
        "You will work with two new classes in this notebook. These are named `BotPlayerEnv` and `PolicyPlayer`. These classes are described below.\n",
        "\n",
        "The `BotPlayerEnv` class provides an interface that can be used with reinforcement learning algorithms to train agents to play games by having them complete against a \"bot player\" controlled by an adversarial search agent (such as `RandomPlayer`). Instances of `BotPlayerEnv` combine an instance of a game environment with an instance of an adversarial agent to create an environment that can be use with our RL algorithms. When an action is taken in this environment, the `BotPlayerEnv` class will apply that action, and then generate and apply an action for the bot player. The code block below demonstrates how to create an instance of `BotPlayerEnv` and how to use it with an instance of `TDAgent`.\n",
        "\n",
        "    nim = Nim(piles=3, stones=9, limit=5)\n",
        "    bot = RandomPlayer('Bot')\n",
        "    bot_env = BotPlayerEnv(game_env=nim, agent=bot)\n",
        "    td = TDAgent(bot_env, gamma=1, random_state=1)\n",
        "\n",
        "\n",
        "\n",
        "An instance of the `PolicyPlayer` class represents an adversarial search agent that follows a policy that maps game states to actions. We will use this class to create agents that follow the policies learned by applying the Q-learning algorithm. It is interesting to note that since `PolicyPlayer` agents don't have to perform a search when selecting actions, they will always select their actions very quickly. It might take a significant amount of time to run the Q-learning algorithm that learns the policy to use in conjunction with `PolicyPlayer`, but once the policy is learned, the agent will play very quickly.\n",
        "\n",
        "The code block below demonstrates how to create an instance of `PolicyPlayer`.\n",
        "\n",
        "    p1 = PolicyPlayer('Policy Player', policy=some_policy)\n",
        "\n"
      ],
      "metadata": {
        "id": "tHzbUBngvyIh"
      },
      "id": "tHzbUBngvyIh"
    },
    {
      "cell_type": "markdown",
      "id": "232d63b0-e508-4d0d-84db-b79508b2a568",
      "metadata": {
        "tags": [],
        "id": "232d63b0-e508-4d0d-84db-b79508b2a568"
      },
      "source": [
        "## Part 1: Basic Q-Learning Agent\n",
        "\n",
        "In Part 1, we will use Q-learning to learn a policy for playing Nim. The policy will be learned by having the Q-learning algorithm play many games against a `RandomPlayer` agent, and will be tested by having it play against `RandomPlayer` and `MinimaxPlayer` agents. Our eventual goal is to find a policy that can be used to consistently defeat a Minimax agent with a depth of 4.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.A - Training the Agent\n",
        "\n",
        "Create the following objects:\n",
        "\n",
        "- An instance of `Nim` with 3 piles, 9 stones per pile, and with a limit of 5 stones per action.\n",
        "- An instance of `RandomPlayer`.\n",
        "- An instance of `BotPlayerEnv` using the `Nim` and `RandomPlayer` instances you created above.\n",
        "- An instance of `TDAgent` that uses your instance of `BotPlayerEnv`. Set `gamma=1` and `random_state=1`.\n",
        "\n",
        "After creating the objects above, use your `TDAgent` instance to apply Q-learning to learn a policy for `Nim`. Run 10,000 episodes of Q-learning with an exploration rate of 0.1 and a learning rate of 0.1. Also set `track_history=False` when calling calling the `q_learning()` method. This will significantly reduce the memory requirements of the algorithm."
      ],
      "metadata": {
        "id": "xaQ9nmX2wivb"
      },
      "id": "xaQ9nmX2wivb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ba9a53c-47db-4bf3-981f-666cf4a9c103",
      "metadata": {
        "id": "7ba9a53c-47db-4bf3-981f-666cf4a9c103"
      },
      "outputs": [],
      "source": [
        "nim = Nim(piles=3, stones=9, limit=5)\n",
        "bot = RandomPlayer('Bot')\n",
        "bot_env = BotPlayerEnv(game_env=nim, agent=bot)\n",
        "td = TDAgent(bot_env, gamma=1, random_state=1)\n",
        "\n",
        "td.q_learning(episodes=10000, epsilon=0.1, alpha=0.1, track_history=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.B - Create Agents\n",
        "\n",
        "Create the following agents:\n",
        "- A `PolicyPlayer` instance using the policy found by Q-learning.\n",
        "- A `RandomPlayer` instance.\n",
        "- A `MinimaxPlayer` instance with `depth=2`.\n",
        "- A `MinimaxPlayer` instance with `depth=3`.\n",
        "- A `MinimaxPlayer` instance with `depth=4`."
      ],
      "metadata": {
        "id": "2LDS0D4Oxmfu"
      },
      "id": "2LDS0D4Oxmfu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da2f49b-d89b-4d81-bbb6-c98451869562",
      "metadata": {
        "id": "4da2f49b-d89b-4d81-bbb6-c98451869562"
      },
      "outputs": [],
      "source": [
        "policyplayer = PolicyPlayer('Policy Player', policy=td.policy)\n",
        "\n",
        "randomplayer = RandomPlayer('Random Player')\n",
        "\n",
        "minimax_player1 = MinimaxPlayer('Minimax Player (Depth 2)', depth=2)\n",
        "minimax_player2 = MinimaxPlayer('Minimax Player (Depth 3)', depth=3)\n",
        "minimax_player3 = MinimaxPlayer('Minimax Player (Depth 4)', depth=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.C - Versus RandomPlayer\n",
        "\n",
        "Run a 1000 round tournament between the `PolicyPlayer` agent and the `RandomPlayer` agent. Set `random_state=1`. When creating the agent list, please list the `PolicyPlayer` agent first."
      ],
      "metadata": {
        "id": "C7aDi23axyA3"
      },
      "id": "C7aDi23axyA3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19483d90-ab77-4b77-bf55-be1488b847a0",
      "metadata": {
        "id": "19483d90-ab77-4b77-bf55-be1488b847a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d46024c-a5bb-4238-81aa-984382f8cf19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 5350.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Player vs. Random Player\n",
            "-------------------------------\n",
            "Ties:          0\n",
            "Player 1 Wins: 920\n",
            "Player 2 Wins: 80\n",
            "Player 1 took: 0.04 seconds\n",
            "Player 2 took: 0.10 seconds\n",
            "Average number of turns: 11.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tournament(nim, agents = [policyplayer, randomplayer], rounds = 1000, random_state = 1, switch_players = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.D - Versus Minimax(2)\n",
        "\n",
        "Run a 1000 round tournament between the `PolicyPlayer` agent and the `MinimaxPlayer` agent with `depth=2`. Set `random_state=1`. When creating the agent list, please list the `PolicyPlayer` agent first."
      ],
      "metadata": {
        "id": "eX9Z39LIyCp7"
      },
      "id": "eX9Z39LIyCp7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d29500a-5a15-4dad-b4ed-b8a3915e93b8",
      "metadata": {
        "id": "9d29500a-5a15-4dad-b4ed-b8a3915e93b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e93a89-733e-4351-eec8-500a611ba37b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 554.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Player vs. Minimax Player (Depth 2)\n",
            "------------------------------------------\n",
            "Ties:          0\n",
            "Player 1 Wins: 726\n",
            "Player 2 Wins: 274\n",
            "Player 1 took: 0.05 seconds\n",
            "Player 2 took: 1.66 seconds\n",
            "Average number of turns: 11.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tournament(nim, agents = [policyplayer, minimax_player1], rounds = 1000, random_state = 1, switch_players = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.E - Versus Minimax(3)\n",
        "\n",
        "Run a 1000 round tournament between the `PolicyPlayer` agent and the `MinimaxPlayer` agent with `depth=3`. Set `random_state=1`. When creating the agent list, please list the `PolicyPlayer` agent first.\n",
        "\n"
      ],
      "metadata": {
        "id": "ily3krHeyMXB"
      },
      "id": "ily3krHeyMXB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c57edd7-fa05-4de4-80ae-20b8f3cb8897",
      "metadata": {
        "id": "2c57edd7-fa05-4de4-80ae-20b8f3cb8897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a634136d-ebcc-4b96-bbb5-982f9e4e77da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:17<00:00, 56.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Player vs. Minimax Player (Depth 3)\n",
            "------------------------------------------\n",
            "Ties:          0\n",
            "Player 1 Wins: 402\n",
            "Player 2 Wins: 598\n",
            "Player 1 took: 0.07 seconds\n",
            "Player 2 took: 17.29 seconds\n",
            "Average number of turns: 12.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tournament(nim, agents = [policyplayer, minimax_player2], rounds = 1000, random_state = 1, switch_players = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.F - Versus Minimax(4)\n",
        "\n",
        "Run a 1000 round tournament between the `PolicyPlayer` agent and the `MinimaxPlayer` agent with `depth=4`. Set `random_state=1`. When creating the agent list, please list the `PolicyPlayer` agent first."
      ],
      "metadata": {
        "id": "7BLjHkZEyQ0L"
      },
      "id": "7BLjHkZEyQ0L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c89c760-4c14-4fe3-b800-14f7f9249f04",
      "metadata": {
        "id": "6c89c760-4c14-4fe3-b800-14f7f9249f04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402e4159-175f-4ac9-aa56-cbd216fa9869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [02:55<00:00,  5.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Player vs. Minimax Player (Depth 4)\n",
            "------------------------------------------\n",
            "Ties:          0\n",
            "Player 1 Wins: 188\n",
            "Player 2 Wins: 812\n",
            "Player 1 took: 0.13 seconds\n",
            "Player 2 took: 173.90 seconds\n",
            "Average number of turns: 11.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tournament(nim, agents = [policyplayer, minimax_player3], rounds = 1000, random_state = 1, switch_players = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.G - Summarizing Results\n",
        "\n",
        "Indicate the win rates for the `PolicyPlayer` agent by filling in each of the blanks below. Proivde your answer as percentages rounded to 1 decimal place."
      ],
      "metadata": {
        "id": "83IIyPnYyXRn"
      },
      "id": "83IIyPnYyXRn"
    },
    {
      "cell_type": "markdown",
      "id": "59a67a38-f5c1-49dc-ba3d-61ea46fe625b",
      "metadata": {
        "id": "59a67a38-f5c1-49dc-ba3d-61ea46fe625b"
      },
      "source": [
        "The policy player won:\n",
        "- ____% of games played against the `RandomPlayer` agent.\n",
        "- ____% of games played against the `MinimaxPlayer` agent with depth 2.\n",
        "- ____% of games played against the `MinimaxPlayer` agent with depth 3.\n",
        "- ____% of games played against the `MinimaxPlayer` agent with depth 4."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce5e992d-7005-4957-8733-23c5e33244f4",
      "metadata": {
        "tags": [],
        "id": "ce5e992d-7005-4957-8733-23c5e33244f4"
      },
      "source": [
        "# Part 2: Improved Q-Learning Agent\n",
        "\n",
        "We will now attempt to improve the `PolicyPlayer` agent by using Q-learning to find a better policy.\n",
        "\n",
        "Please repeat steps A-G from Part 1, with one change. You should experiment with the parameters used in Part A when applying Q-learning. You can experiment with any or all of the following:\n",
        "\n",
        "* You can increase the number of episodes.\n",
        "* You can experiment with the exploration rate.\n",
        "* You can experiment with the learning rate.\n",
        "* You can experiment with exploring starts.\n",
        "* You can consider applying multiple rounds of Q-learning, changing the parameters between the two rounds.\n",
        "\n",
        "Your goal is to find a policy that results in the `PolicyPlayer` agent winning at least 85% of the games in the tournament against the `MinimaxPlayer` with `depth=4`. If you find a policy that gets close to this, you will get partial credit, but you will not receive full credit unless the `PolicyPlayer` agent reaches the 85% win rate.\n",
        "\n",
        "**Hint:** You should be able to attain this goal by simply increasing the number of episodes. However, you might be able to achieve the desired results in fewer episodes by experimenting with the other parameters. You also might be able to get an agent that gets a win rate higher than 85% by experimenting with the other parameters. It is possible (but not required) to find a policy with a win rate of at least 95%."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.A - Training the Agent\n",
        "\n",
        "Create the following objects:\n",
        "\n",
        "- An instance of `Nim` with 3 piles, 9 stones per pile, and with a limit of 5 stones per action.\n",
        "- An instance of `RandomPlayer`.\n",
        "- An instance of `BotPlayerEnv` using the `Nim` and `RandomPlayer` instances you created above.\n",
        "- An instance of `TDAgent` that uses your instance of `BotPlayerEnv`. Set `gamma=1` and `random_state=1`.\n",
        "\n",
        "After creating the objects above, use your `TDAgent` instance to apply Q-learning to learn a policy for `Nim`. As before, you should set `track_history=False` when calling the `q_learning()` method."
      ],
      "metadata": {
        "id": "8UqAsJo4zsL8"
      },
      "id": "8UqAsJo4zsL8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe2cdeac-5596-4372-b2fe-8ebd8645cb1b",
      "metadata": {
        "id": "fe2cdeac-5596-4372-b2fe-8ebd8645cb1b"
      },
      "outputs": [],
      "source": [
        "nim_game = Nim(piles=3, stones=9, limit=5)\n",
        "\n",
        "random_player = RandomPlayer('Random Player 2')\n",
        "\n",
        "bot_env = BotPlayerEnv(game_env=nim_game, agent=random_player)\n",
        "\n",
        "td2 = TDAgent(bot_env, gamma=1, random_state=1)\n",
        "\n",
        "td2.q_learning(episodes=50000, epsilon=0.1, alpha=0.1, track_history=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.B - Create Agents\n",
        "\n",
        "Create the following agents:\n",
        "- A `PolicyPlayer` instance using the policy found by Q-learning.\n",
        "- A `RandomPlayer` instance.\n",
        "- A `MinimaxPlayer` instance with `depth=2`.\n",
        "- A `MinimaxPlayer` instance with `depth=3`.\n",
        "- A `MinimaxPlayer` instance with `depth=4`."
      ],
      "metadata": {
        "id": "dalb8tqJ0AaP"
      },
      "id": "dalb8tqJ0AaP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa144d9a-9669-41aa-8874-752880c4b097",
      "metadata": {
        "id": "aa144d9a-9669-41aa-8874-752880c4b097"
      },
      "outputs": [],
      "source": [
        "policyplayer2 = PolicyPlayer('Policy Player', policy=td2.policy)\n",
        "\n",
        "random_player = RandomPlayer('Random Player')\n",
        "\n",
        "minimaxplayer_depth_2 = MinimaxPlayer('Minimax Player Depth 2', depth=2)\n",
        "minimaxplayer_depth_3 = MinimaxPlayer('Minimax Player Depth 3', depth=3)\n",
        "minimaxplayer_depth_4 = MinimaxPlayer('Minimax Player Depth 4', depth=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.C - Versus RandomPlayer\n",
        "\n",
        "Run a 1000 round tournament between the `PolicyPlayer` agent and the `RandomPlayer` agent. Set `random_state=1`. When creating the agent list, please list the `PolicyPlayer` agent first."
      ],
      "metadata": {
        "id": "kM6IwYZQ0Fwu"
      },
      "id": "kM6IwYZQ0Fwu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34cc53bb-981a-48b2-af5e-8aa6d5946e55",
      "metadata": {
        "id": "34cc53bb-981a-48b2-af5e-8aa6d5946e55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db8187b0-0a4d-4ab5-ab74-e8f5fe6a23d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 6272.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Player vs. Random Player\n",
            "-------------------------------\n",
            "Ties:          0\n",
            "Player 1 Wins: 968\n",
            "Player 2 Wins: 32\n",
            "Player 1 took: 0.03 seconds\n",
            "Player 2 took: 0.09 seconds\n",
            "Average number of turns: 11.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tournament(nim_game, agents = [policyplayer2, random_player],rounds=1000,  random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.D - Versus Minimax(2)\n",
        "\n",
        "Run a 1000 round tournament between the `PolicyPlayer` agent and the `MinimaxPlayer` agent with `depth=2`. Set `random_state=1`. When creating the agent list, please list the `PolicyPlayer` agent first."
      ],
      "metadata": {
        "id": "R-4PjeKA0IXo"
      },
      "id": "R-4PjeKA0IXo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c43e43-ad3e-4fbf-977c-3b7af7e01a8f",
      "metadata": {
        "id": "b9c43e43-ad3e-4fbf-977c-3b7af7e01a8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52059dea-8c1b-4a9d-90c6-1ec624dd2dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 526.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Player vs. Minimax Player Depth 2\n",
            "----------------------------------------\n",
            "Ties:          0\n",
            "Player 1 Wins: 865\n",
            "Player 2 Wins: 135\n",
            "Player 1 took: 0.06 seconds\n",
            "Player 2 took: 1.73 seconds\n",
            "Average number of turns: 11.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tournament(nim_game, agents = [policyplayer2, minimaxplayer_depth_2],rounds=1000,  random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.E - Versus Minimax(3)\n",
        "\n",
        "Run a 1000 round tournament between the `PolicyPlayer` agent and the `MinimaxPlayer` agent with `depth=3`. Set `random_state=1`. When creating the agent list, please list the `PolicyPlayer` agent first.\n",
        "\n"
      ],
      "metadata": {
        "id": "_ck3axAb0Kad"
      },
      "id": "_ck3axAb0Kad"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15ac05d8-3933-4dd1-a62d-9d232957844e",
      "metadata": {
        "id": "15ac05d8-3933-4dd1-a62d-9d232957844e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b2814c6-20db-40cf-869a-26d2570f1b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:18<00:00, 55.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Player vs. Minimax Player Depth 3\n",
            "----------------------------------------\n",
            "Ties:          0\n",
            "Player 1 Wins: 607\n",
            "Player 2 Wins: 393\n",
            "Player 1 took: 0.07 seconds\n",
            "Player 2 took: 17.62 seconds\n",
            "Average number of turns: 12.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tournament(nim_game, agents = [policyplayer2, minimaxplayer_depth_3],rounds=1000,  random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.F - Versus Minimax(4)\n",
        "\n",
        "Run a 1000 round tournament between the `PolicyPlayer` agent and the `MinimaxPlayer` agent with `depth=4`. Set `random_state=1`. When creating the agent list, please list the `PolicyPlayer` agent first."
      ],
      "metadata": {
        "id": "4ExhhW1y0Msu"
      },
      "id": "4ExhhW1y0Msu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea2764d-15d3-4f82-bbc0-f1a1a7f63f95",
      "metadata": {
        "id": "7ea2764d-15d3-4f82-bbc0-f1a1a7f63f95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d6c4b2e-c074-4d59-a8c6-62edaa119b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [02:53<00:00,  5.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Player vs. Minimax Player Depth 4\n",
            "----------------------------------------\n",
            "Ties:          0\n",
            "Player 1 Wins: 362\n",
            "Player 2 Wins: 638\n",
            "Player 1 took: 0.13 seconds\n",
            "Player 2 took: 171.95 seconds\n",
            "Average number of turns: 11.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tournament(nim_game, agents = [policyplayer2, minimaxplayer_depth_4],rounds=1000,  random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.G - Summarizing Results\n",
        "\n",
        "Indicate the win rates for the `PolicyPlayer` agent by filling in each of the blanks below. Proivde your answer as percentages rounded to 1 decimal place."
      ],
      "metadata": {
        "id": "SvmqVM200RG3"
      },
      "id": "SvmqVM200RG3"
    },
    {
      "cell_type": "markdown",
      "id": "8f212fa7-2d46-421e-88c8-1d7c7e64ffec",
      "metadata": {
        "id": "8f212fa7-2d46-421e-88c8-1d7c7e64ffec"
      },
      "source": [
        "The policy player won:\n",
        "- ____% of games played against the `RandomPlayer` agent.\n",
        "- ____% of games played against the `MinimaxPlayer` agent with depth 2.\n",
        "- ____% of games played against the `MinimaxPlayer` agent with depth 3.\n",
        "- ____% of games played against the `MinimaxPlayer` agent with depth 4."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission Instructions\n",
        "\n",
        "1. Perform a Restart and Run All by clicking **Tools > Restart session and run all**.\n",
        "2. Copy the link to your notebook by clicking **Share > Copy Link**.\n",
        "3. Paste the copied link into the `notebook_url` field in the [Notebook Renderer](https://colab.research.google.com/drive/1CJTipys46ldZxJFwnt7XbdjQUfkmoXeU?usp=sharing) tool and then execute the cell to render the notebook.\n",
        "4. The Notebook Renderer will open up a save file dialog. Save the resulting HTML file yo your local machine.\n",
        "5. Submit the HTML file to Canvas.\n"
      ],
      "metadata": {
        "id": "c1yM8qwP0cX2"
      },
      "id": "c1yM8qwP0cX2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}