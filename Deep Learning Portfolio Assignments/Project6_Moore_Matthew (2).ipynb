{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7227cd",
   "metadata": {},
   "source": [
    "# Project 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526152b",
   "metadata": {},
   "source": [
    "## Assignment Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f47a1",
   "metadata": {},
   "source": [
    "You are a data scientist in a call center. You are given a tab-delimited bilingual sentence pair contained in the file, deu-eng.txt Download deu-eng.txt.\n",
    "\n",
    "The dataset has the following format:\n",
    "\n",
    "English + TAB + German + TAB + Attribution\n",
    "\n",
    "You are asked to develop a sequence to sequence model using RNN to translate English to German using the attached Jupyter Notebook and writing a script in Python, and running all the cells. You only need to submit a Jupyter Notebook.\n",
    "\n",
    "**Steps and Questions**\n",
    "\n",
    "1. Load the data, deu-eng.txt Download deu-eng.txt, into memory and clean and prepare the data.\n",
    "\n",
    "2. Develop an encoder for the model.\n",
    "\n",
    "3. Develop a decoder for the model.\n",
    "\n",
    "4. Build the sequence to sequence model.\n",
    "\n",
    "5. Train the model.\n",
    "\n",
    "6. Evaluate the model and check the model fit.\n",
    "\n",
    "7. Run the inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b4629",
   "metadata": {},
   "source": [
    "## Load the Necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e496de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling and preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tokenization and padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Model building\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Model training\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e89834",
   "metadata": {},
   "source": [
    "## Step 1: Load, Clean, and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f2c9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 85\n",
      "Max sequence length for inputs: 15\n",
      "Max sequence length for outputs: 45\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Set file path\n",
    "data_path = \"/Users/matthewmoore/Downloads/deu-eng.txt\"\n",
    "\n",
    "# Initialize variables\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "# Read and process the data\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "\n",
    "for line in lines[:10000]:  # Limit the number of samples to 10,000\n",
    "    if \"\\t\" not in line:\n",
    "        continue\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "\n",
    "    for char in input_text:\n",
    "        input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        target_characters.add(char)\n",
    "\n",
    "# Sort the characters\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "\n",
    "# Define vocabulary sizes and sequence lengths\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print(f\"Number of samples: {len(input_texts)}\")\n",
    "print(f\"Number of unique input tokens: {num_encoder_tokens}\")\n",
    "print(f\"Number of unique output tokens: {num_decoder_tokens}\")\n",
    "print(f\"Max sequence length for inputs: {max_encoder_seq_length}\")\n",
    "print(f\"Max sequence length for outputs: {max_decoder_seq_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6688e01",
   "metadata": {},
   "source": [
    "## Step 2-3: Develop an Encoder and Decoder for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c7c098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries for character-to-index and index-to-character mapping\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "target_char_index = {char: i for i, char in enumerate(sorted(list(target_characters)))}\n",
    "reverse_target_char_index = {i: char for char, i in target_char_index.items()}\n",
    "\n",
    "# Initialize data arrays\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "\n",
    "# Populate the data arrays\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853bcdec",
   "metadata": {},
   "source": [
    "## Step 4: Develop the Sequence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec145ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions\n",
    "latent_dim = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Combined model\n",
    "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2664f95",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90868789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 15s 103ms/step - loss: 1.4303 - accuracy: 0.0525 - val_loss: 1.4847 - val_accuracy: 0.0740\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.2982 - accuracy: 0.0616 - val_loss: 1.4456 - val_accuracy: 0.0834\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.2741 - accuracy: 0.0650 - val_loss: 1.4355 - val_accuracy: 0.0880\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.2524 - accuracy: 0.0691 - val_loss: 1.3949 - val_accuracy: 0.0861\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.2331 - accuracy: 0.0752 - val_loss: 1.3996 - val_accuracy: 0.1047\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.2154 - accuracy: 0.0832 - val_loss: 1.3473 - val_accuracy: 0.1106\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.1989 - accuracy: 0.0896 - val_loss: 1.3340 - val_accuracy: 0.1157\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.1821 - accuracy: 0.1000 - val_loss: 1.3233 - val_accuracy: 0.1268\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.1644 - accuracy: 0.1060 - val_loss: 1.3035 - val_accuracy: 0.1358\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.1479 - accuracy: 0.1108 - val_loss: 1.2844 - val_accuracy: 0.1413\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.1316 - accuracy: 0.1145 - val_loss: 1.2706 - val_accuracy: 0.1423\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 12s 98ms/step - loss: 1.1169 - accuracy: 0.1181 - val_loss: 1.2426 - val_accuracy: 0.1507\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.1041 - accuracy: 0.1213 - val_loss: 1.2541 - val_accuracy: 0.1448\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.0925 - accuracy: 0.1248 - val_loss: 1.2240 - val_accuracy: 0.1559\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.0781 - accuracy: 0.1285 - val_loss: 1.3304 - val_accuracy: 0.1186\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.0702 - accuracy: 0.1306 - val_loss: 1.2154 - val_accuracy: 0.1590\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.0602 - accuracy: 0.1336 - val_loss: 1.1965 - val_accuracy: 0.1634\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.0508 - accuracy: 0.1357 - val_loss: 1.2126 - val_accuracy: 0.1606\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.0429 - accuracy: 0.1380 - val_loss: 1.1976 - val_accuracy: 0.1628\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.0349 - accuracy: 0.1404 - val_loss: 1.1720 - val_accuracy: 0.1693\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.0281 - accuracy: 0.1422 - val_loss: 1.1756 - val_accuracy: 0.1702\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 12s 100ms/step - loss: 1.0206 - accuracy: 0.1445 - val_loss: 1.1510 - val_accuracy: 0.1777\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 1.0134 - accuracy: 0.1466 - val_loss: 1.1605 - val_accuracy: 0.1741\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 15s 119ms/step - loss: 1.0076 - accuracy: 0.1483 - val_loss: 1.1560 - val_accuracy: 0.1758\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 17s 133ms/step - loss: 0.9998 - accuracy: 0.1502 - val_loss: 1.1558 - val_accuracy: 0.1755\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 17s 133ms/step - loss: 0.9935 - accuracy: 0.1519 - val_loss: 1.1707 - val_accuracy: 0.1761\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 14s 108ms/step - loss: 0.9874 - accuracy: 0.1536 - val_loss: 1.1114 - val_accuracy: 0.1838\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 15s 117ms/step - loss: 0.9816 - accuracy: 0.1548 - val_loss: 1.1411 - val_accuracy: 0.1802\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 13s 103ms/step - loss: 0.9759 - accuracy: 0.1564 - val_loss: 1.1169 - val_accuracy: 0.1830\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 13s 102ms/step - loss: 0.9711 - accuracy: 0.1574 - val_loss: 1.1021 - val_accuracy: 0.1883\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 13s 102ms/step - loss: 0.9665 - accuracy: 0.1585 - val_loss: 1.1214 - val_accuracy: 0.1847\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 13s 103ms/step - loss: 0.9611 - accuracy: 0.1598 - val_loss: 1.0960 - val_accuracy: 0.1877\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 13s 103ms/step - loss: 0.9552 - accuracy: 0.1610 - val_loss: 1.1076 - val_accuracy: 0.1890\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 13s 103ms/step - loss: 0.9519 - accuracy: 0.1620 - val_loss: 1.0818 - val_accuracy: 0.1927\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 13s 102ms/step - loss: 0.9476 - accuracy: 0.1627 - val_loss: 1.0957 - val_accuracy: 0.1889\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 0.9432 - accuracy: 0.1637 - val_loss: 1.0800 - val_accuracy: 0.1901\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 13s 100ms/step - loss: 0.9389 - accuracy: 0.1645 - val_loss: 1.0827 - val_accuracy: 0.1913\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 13s 100ms/step - loss: 0.9350 - accuracy: 0.1653 - val_loss: 1.0615 - val_accuracy: 0.1942\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 0.9311 - accuracy: 0.1663 - val_loss: 1.0779 - val_accuracy: 0.1918\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 13s 100ms/step - loss: 0.9266 - accuracy: 0.1675 - val_loss: 1.0660 - val_accuracy: 0.1948\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 0.9233 - accuracy: 0.1682 - val_loss: 1.1003 - val_accuracy: 0.1903\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 13s 100ms/step - loss: 0.9206 - accuracy: 0.1686 - val_loss: 1.0720 - val_accuracy: 0.1941\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 0.9162 - accuracy: 0.1700 - val_loss: 1.0620 - val_accuracy: 0.1973\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 0.9125 - accuracy: 0.1709 - val_loss: 1.0502 - val_accuracy: 0.1979\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 0.9098 - accuracy: 0.1713 - val_loss: 1.0575 - val_accuracy: 0.1975\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 0.9067 - accuracy: 0.1722 - val_loss: 1.0630 - val_accuracy: 0.1983\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 0.9036 - accuracy: 0.1733 - val_loss: 1.0306 - val_accuracy: 0.2006\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 0.9001 - accuracy: 0.1740 - val_loss: 1.0387 - val_accuracy: 0.1995\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 0.8979 - accuracy: 0.1748 - val_loss: 1.0694 - val_accuracy: 0.1967\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 0.8938 - accuracy: 0.1758 - val_loss: 1.0443 - val_accuracy: 0.1996\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "training_model.compile(optimizer=RMSprop(learning_rate=0.0005), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = training_model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ae3403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewmoore/opt/anaconda3/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model for future inference\n",
    "training_model.save(\"seq2seq_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b94c2",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate the Model and Check Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eafa3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 7s 22ms/step - loss: 0.9191 - accuracy: 0.1813\n",
      "Validation Loss: 0.9191\n",
      "Validation Accuracy: 0.1813\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on validation data\n",
    "val_loss, val_accuracy = training_model.evaluate(\n",
    "    [encoder_input_data, decoder_input_data], decoder_target_data, verbose=1\n",
    ")\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442b579",
   "metadata": {},
   "source": [
    "## Step 7: Run the Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "594f50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder model\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e47cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input sequence to get the context vector\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate the start-of-sequence token\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.0\n",
    "\n",
    "    # Initialize the decoded sentence\n",
    "    decoded_sentence = \"\"\n",
    "    stop_condition = False\n",
    "\n",
    "    while not stop_condition:\n",
    "        # Predict the next token\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample the token with the highest probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Check for stop condition\n",
    "        if sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence with the sampled token\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update the states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "809434dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 271ms/step\n",
      "1/1 [==============================] - 0s 255ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "--------------------------------------------------\n",
      "Input Sentence: Go.\n",
      "Decoded Sentence: Gen eeneneeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "Target Sentence: \tGeh.\n",
      "\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "--------------------------------------------------\n",
      "Input Sentence: Hi.\n",
      "Decoded Sentence: Alleeeneeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "Target Sentence: \tHallo!\n",
      "\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "--------------------------------------------------\n",
      "Input Sentence: Hi.\n",
      "Decoded Sentence: Alleeeneeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "Target Sentence: \tGrüß Gott!\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "--------------------------------------------------\n",
      "Input Sentence: Run!\n",
      "Decoded Sentence: Alleeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "Target Sentence: \tLauf!\n",
      "\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "--------------------------------------------------\n",
      "Input Sentence: Run.\n",
      "Decoded Sentence: Ger eineeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "Target Sentence: \tLauf!\n",
      "\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "--------------------------------------------------\n",
      "Input Sentence: Wow!\n",
      "Decoded Sentence: Mar eineeneeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "Target Sentence: \tPotzdonner!\n",
      "\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "--------------------------------------------------\n",
      "Input Sentence: Wow!\n",
      "Decoded Sentence: Mar eineeneeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "Target Sentence: \tDonnerwetter!\n",
      "\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "--------------------------------------------------\n",
      "Input Sentence: Fire!\n",
      "Decoded Sentence: Ger eine eineeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "Target Sentence: \tFeuer!\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "--------------------------------------------------\n",
      "Input Sentence: Help!\n",
      "Decoded Sentence: Blleeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "Target Sentence: \tHilfe!\n",
      "\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "--------------------------------------------------\n",
      "Input Sentence: Help!\n",
      "Decoded Sentence: Blleeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "Target Sentence: \tZu Hülf!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate and decode predictions for 10 sequences\n",
    "for seq_index in range(10):\n",
    "    # Take one sequence from the test or validation set for decoding\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "    \n",
    "    # Use the decoder to predict the sequence\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Input Sentence: {input_texts[seq_index]}\")\n",
    "    print(f\"Decoded Sentence: {decoded_sentence}\")\n",
    "    print(f\"Target Sentence: {target_texts[seq_index]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
